/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
Loading dataset...
{'image_id': 'mol_12599933', 'file_path': 'indigo_simple_render/images/mol_12599933.png', 'SMILES': 'CC[Si](CC[Si](CC)(Cl)Cl)(Cl)Cl', 'hydrogen_atom_count': 14, 'has_aromatic_ring': False, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=722x277 at 0x7F3C34BCB6A0>}
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.36s/it]
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: flashsonic (flashsonic-mit) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.5
wandb: Run data is saved locally in /data/scratch/richwang/.cache/wandb/run-20250301_163501-6ytcu7ep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen2-VL-2B-GRPO-tester
wandb: ⭐️ View project at https://wandb.ai/flashsonic-mit/huggingface
wandb: 🚀 View run at https://wandb.ai/flashsonic-mit/huggingface/runs/6ytcu7ep
  0%|          | 0/7500 [00:00<?, ?it/s][16:35:08] SMILES Parse Error: unclosed ring for input: 'CCl=CCl=NOC1'
[16:35:08] SMILES Parse Error: syntax error while parsing: [Oc1ccccc1]
[16:35:08] SMILES Parse Error: check for mistakes around position 3:
[16:35:08] [Oc1ccccc1]
[16:35:08] ~~^
[16:35:08] SMILES Parse Error: Failed parsing SMILES '[Oc1ccccc1]' for input: '[Oc1ccccc1]'
/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
Traceback (most recent call last):
  File "/data/rbg/users/richwang/trl/experiments.py", line 98, in <module>
    trainer.train()
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/transformers/trainer.py", line 2577, in _inner_training_loop
    self.optimizer.step()
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
    return func(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 767, in adamw
    func(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 600, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 22.25 MiB is free. Including non-PyTorch memory, this process has 47.50 GiB memory in use. Of the allocated memory 46.03 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/data/rbg/users/richwang/trl/experiments.py", line 98, in <module>
    trainer.train()
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/transformers/trainer.py", line 2577, in _inner_training_loop
    self.optimizer.step()
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
    return func(*args, **kwargs)
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 767, in adamw
    func(
  File "/data/rbg/users/richwang/nlp/visionrl/lib/python3.9/site-packages/torch/optim/adamw.py", line 600, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 22.25 MiB is free. Including non-PyTorch memory, this process has 47.50 GiB memory in use. Of the allocated memory 46.03 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mQwen2-VL-2B-GRPO-tester[0m at: [34mhttps://wandb.ai/flashsonic-mit/huggingface/runs/6ytcu7ep[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../scratch/richwang/.cache/wandb/run-20250301_163501-6ytcu7ep/logs[0m
